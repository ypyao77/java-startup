package org.demo.hadoop.parquet.remote;

import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.security.UserGroupInformation;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import parquet.avro.AvroParquetWriter;
import parquet.example.data.Group;
import parquet.example.data.simple.SimpleGroupFactory;
import parquet.hadoop.ParquetWriter;
import parquet.hadoop.example.GroupWriteSupport;
import parquet.hadoop.metadata.CompressionCodecName;
import parquet.schema.MessageType;
import parquet.schema.MessageTypeParser;

import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.Map;

public class SchemaParquetFileWrite {
    private static final Logger LOG = LoggerFactory.getLogger(SchemaParquetFileWrite.class);
    private static String PARQUET_FILE = "hdfs://fhc/data/arch/tmp/test/s_test_namecs_temp/names.parq";
    private static String KERB_REALM;
    private static String KERB_FILE;
    private static String KERB_CONF;

    static {
        if (System.getProperty("os.name").toLowerCase().startsWith("win")) {
            System.setProperty("hadoop.home.dir", "E:\\hadoop\\winutils\\hadoop-2.6.0");

            KERB_REALM = "arch_onedata@OD.BETA";
            KERB_FILE = "D:\\kerberos\\arch_onedata.keytab.beta";
            KERB_CONF = "D:\\kerberos\\krb5-beta.conf";
        } else {
            KERB_REALM = "arch_onedata@OD.BETA";
            KERB_FILE = "/home/hadoop/code/temp/arch_onedata.keytab";
            KERB_CONF = "/etc/krb5.conf";
        }
    }

    public static void main(String[] args) {
        if (args.length > 0) {
            PARQUET_FILE = args[0];
        }

        // First thing - parse the schema as it will be used
        System.out.println("main(): ");
        writeToParquet();
    }

    private static void writeToParquet() {
        System.out.println("writeToParquet()");

        MessageType schema = MessageTypeParser.parseMessageType(
                "message AutoGeneratedSchema { " +
                        "optional binary ETLTIME (UTF8);" +
                        "optional binary PKVIRT;" +
                        "optional binary CT (UTF8);" +
                        "optional int64 ID;" +
                        "optional binary NAME (UTF8);" +
                        "optional binary CTIME (UTF8);" +
                        "optional binary MASK (UTF8);" +
                        "}"
        );
        SimpleGroupFactory sfg = new SimpleGroupFactory(schema);

        // Path to Parquet file in HDFS
        Path path = new Path(PARQUET_FILE);
        // ParquetWriter<GenericData.Record> writer = null;
        ParquetWriter<Group> writer = null;

        // Creating ParquetWriter using builder
        try {
            Configuration conf = new Configuration();

            LOG.info("conf.set()");

            conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
            conf.set("fs.file.impl", org.apache.hadoop.fs.LocalFileSystem.class.getName());
            conf.set("fs.webhdfs.impl", org.apache.hadoop.hdfs.web.WebHdfsFileSystem.class.getName());

            // conf.set("dfs.namenode.kerberos.principal","hdfs/_HOST@OD.BETA");
            // conf.set("dfs.datanode.kerberos.principal","hdfs/_HOST@OD.BETA");

            conf.set("hadoop.security.authentication", "kerberos");
            conf.set("hadoop.security.authorization", "true");

            System.setProperty("java.security.krb5.conf", KERB_CONF);

            LOG.info("UserGroupInformation.setConfiguration()");
            UserGroupInformation.setConfiguration(conf);
            LOG.info("UserGroupInformation.loginUserFromKeytab()");
            UserGroupInformation.loginUserFromKeytab(KERB_REALM, KERB_FILE);
            LOG.info("UserGroupInformation.getLoginUser()");
            UserGroupInformation.getLoginUser();

            conf.set("dfs.client.failover.proxy.provider.fhc", "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider");
            conf.set("dfs.ha.namenodes.fhc", "nn1,nn2");
            conf.set("dfs.namenode.rpc-address.fhc.nn1", "arch-od-data01.beta1.fn:8020");
            conf.set("dfs.namenode.rpc-address.fhc.nn2", "arch-od-data02.beta1.fn:8020");
            conf.set("dfs.nameservices", "fhc");

            GroupWriteSupport.setSchema(schema, conf);
            GroupWriteSupport groupWriteSupport = new GroupWriteSupport();

            writer = new ParquetWriter<Group>(
                    path,
                    groupWriteSupport,
                    CompressionCodecName.SNAPPY,
                    1024,
                    1024,
                    512,
                    true,
                    false,
                    conf);

            for (long i = 0; i < 5; ++i) {
                Group group = sfg.newGroup()
                        .append("ETLTIME", "2018-08-24 12:01:" + String.format("%02d", i))
                        .append("PKVIRT", "3354")
                        .append("CT", "2018-08-24")
                        .append("ID", i)
                        .append("NAME", String.format("姓名%d", i))
                        .append("CTIME", (new java.text.SimpleDateFormat("yyyy-MM-dd hh:mm:ss")).format(new Date()))
                        .append("MASK", "0");
                writer.write(group);
            }
        } catch (IOException e) {
            e.printStackTrace();
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            if (writer != null) {
                try {
                    writer.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
    }
}
