package org.demo.hadoop.parquet.remote;


import java.io.IOException;
import java.util.Date;

import org.apache.hadoop.security.UserGroupInformation;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;

import parquet.column.ParquetProperties;
import parquet.example.data.Group;
import parquet.example.data.simple.SimpleGroup;
import parquet.example.data.simple.SimpleGroupFactory;
import parquet.format.converter.ParquetMetadataConverter;
import parquet.hadoop.ParquetFileReader;
import parquet.hadoop.ParquetFileWriter;
import parquet.hadoop.ParquetReader;
import parquet.hadoop.ParquetWriter;
import parquet.hadoop.api.ReadSupport;
import parquet.hadoop.example.ExampleParquetWriter;
import parquet.hadoop.example.GroupReadSupport;
import parquet.hadoop.metadata.CompressionCodecName;
import parquet.hadoop.metadata.ParquetMetadata;
import parquet.schema.MessageType;
import parquet.schema.MessageTypeParser;


public class TestExampleParquet {
    private static final Logger LOG = LoggerFactory.getLogger(TestExampleParquet.class);
    private static String PARQUET_FILE = "hdfs://fhc/tmp/000000_0";
    private static String PARQUET_FILE_1 = "hdfs://fhc/tmp/000000_1";
    private static String KERB_REALM;
    private static String KERB_FILE;
    private static String KERB_CONF;

    private static String schemaStr =
            "message AutoGeneratedSchema { " +
                    "optional binary ETLTIME (UTF8);" +
                    "optional binary PKVIRT;" +
                    "optional binary CT (UTF8);" +
                    "optional int64 ID;" +
                    "optional binary NAME (UTF8);" +
                    "optional binary CTIME (UTF8);" +
                    "optional binary MASK (UTF8);" +
                    "}";
    static MessageType schema = MessageTypeParser.parseMessageType(schemaStr);

    static {
        if (System.getProperty("os.name").toLowerCase().startsWith("win")) {
            System.setProperty("hadoop.home.dir", "E:\\hadoop\\winutils\\hadoop-2.6.0");

            KERB_REALM = "arch_onedata@OD.BETA";
            KERB_FILE = "D:\\kerberos\\arch_onedata.keytab.beta";
            KERB_CONF = "D:\\kerberos\\krb5-beta.conf";
        } else {
            KERB_REALM = "arch_onedata@OD.BETA";
            KERB_FILE = "/home/hadoop/code/temp/arch_onedata.keytab";
            KERB_CONF = "/etc/krb5.conf";
        }
    }

    public static void main(String[] args) throws Exception {
        if (args.length > 0) {
            PARQUET_FILE = args[0];
        }

        // First thing - parse the schema as it will be used
        LOG.info("main(): ");

        // testGetSchema();
        // testParquetReader();
        testParquetWriter();
    }

    public static Configuration setHadoopConf() throws IOException {
        Configuration conf = new Configuration();

        LOG.info("conf.set()");

        conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
        conf.set("fs.file.impl", org.apache.hadoop.fs.LocalFileSystem.class.getName());
        conf.set("fs.webhdfs.impl", org.apache.hadoop.hdfs.web.WebHdfsFileSystem.class.getName());

        // conf.set("dfs.namenode.kerberos.principal","hdfs/_HOST@OD.BETA");
        // conf.set("dfs.datanode.kerberos.principal","hdfs/_HOST@OD.BETA");

        conf.set("hadoop.security.authentication", "kerberos");
        conf.set("hadoop.security.authorization", "true");

        System.setProperty("java.security.krb5.conf", KERB_CONF);

        LOG.info("UserGroupInformation.setConfiguration()");
        UserGroupInformation.setConfiguration(conf);
        LOG.info("UserGroupInformation.loginUserFromKeytab()");
        UserGroupInformation.loginUserFromKeytab(KERB_REALM, KERB_FILE);
        LOG.info("UserGroupInformation.getLoginUser()");
        UserGroupInformation.getLoginUser();

        conf.set("dfs.client.failover.proxy.provider.fhc", "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider");
        conf.set("dfs.ha.namenodes.fhc", "nn1,nn2");
        conf.set("dfs.namenode.rpc-address.fhc.nn1", "arch-od-data01.beta1.fn:8020");
        conf.set("dfs.namenode.rpc-address.fhc.nn2", "arch-od-data02.beta1.fn:8020");
        conf.set("dfs.nameservices", "fhc");

        return conf;
    }
    public static void testGetSchema() throws Exception {
        Configuration conf = setHadoopConf();

        ParquetMetadata readFooter = null;
        Path parquetFilePath = new Path(PARQUET_FILE);

        readFooter = ParquetFileReader.readFooter(conf, parquetFilePath, ParquetMetadataConverter.NO_FILTER);
        MessageType schema = readFooter.getFileMetaData().getSchema();
        LOG.warn(schema.toString());
    }

    private static void testParquetWriter() throws IOException {
        Configuration conf = setHadoopConf();
        Path file = new Path(PARQUET_FILE_1);

        ExampleParquetWriter.Builder builder = ExampleParquetWriter
                .builder(file).withWriteMode(ParquetFileWriter.Mode.CREATE)
                .withWriterVersion(ParquetProperties.WriterVersion.PARQUET_1_0)
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .withConf(conf)
                .withType(schema);

        ParquetWriter<Group> writer = builder.build();
        SimpleGroupFactory groupFactory = new SimpleGroupFactory(schema);

        for (int i = 0; i < 1000; i++) {
            Group group = groupFactory.newGroup()
                    .append("ETLTIME", "2018-08-24 12:01:" + String.format("%02d", i))
                    .append("PKVIRT", "3354")
                    .append("CT", "2018-08-24")
                    .append("ID", i)
                    .append("NAME", String.format("姓名%d", i))
                    .append("CTIME", (new java.text.SimpleDateFormat("yyyy-MM-dd hh:mm:ss")).format(new Date()))
                    .append("MASK", "0");
            writer.write(group);
        }
        writer.close();
    }

    private static void testParquetReader() throws IOException {
        Configuration conf = setHadoopConf();
        ParquetMetadata readFooter = null;

        Path file = new Path(PARQUET_FILE);
        ParquetReader.Builder<Group> builder = ParquetReader.builder(new GroupReadSupport(), file);
        ParquetReader<Group> reader = builder.withConf(conf).build();
        SimpleGroup group = (SimpleGroup) reader.read();

        LOG.warn("schema:" + group.getType().toString());

        LOG.warn("record:" + group.getString(1, 0));
        LOG.warn("record:" + group.getString(2, 0));
        LOG.warn("record:" + group.getLong(3, 0));
        LOG.warn("record:" + group.getLong(4, 0));
        LOG.warn("record:" + group.getLong(5, 0));
    }
}
